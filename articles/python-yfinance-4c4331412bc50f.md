---
title: "æ—¥æœ¬æ ª3700ç¤¾ä»¥ä¸Šã‚’åˆ†æã€‚yfinance xã€Œã‚ãŒæŠ•è³‡è¡“ã€æ ªå¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ—ãƒªã‚’ä½œã£ãŸè©±ï¼ˆãƒã‚¤ãƒ–ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰"
emoji: "ğŸ“Š"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["python", "yfinance", "react", "docker", "githubactions"]
published: false
---

# ã¯ã˜ã‚ã«/ä½œã£ãŸã‚ã‘

:::message
**ãƒã‚¤ãƒ–ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ä½œã£ãŸã‚¢ãƒ—ãƒªã®è¨˜äº‹ã«ãªã‚Šã¾ã™**
:::

ã„ããªã‚Šã§ã™ãŒã€‚
æµ·å¤–ã§åƒãå§‹ã‚ãŸã‚Šæ—…è¡Œã—ãŸã‚Šã™ã‚‹ã¨ã€æ—¥æœ¬ã®è‰¯ã•ãŒèº«ã«æŸ“ã¿ãŸã¨æ„Ÿã˜ãŸäººã¯å¤šã„ã‚“ã˜ã‚ƒãªã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ
ãªã‚“ã‹ã¨ã‚Šã‚ãˆãšå¤–ã§åƒã„ã¦ã¿ãŸã„ã¨æ€ã£ã¦ã„ã¾ã—ãŸãŒã€ä»Šã¯ã„ã¤æˆ»ã‚‹ã‹ã¨è€ƒãˆã‚‹æ—¥ã€…ã§ã™ã€‚ï¼ˆã¨ã«ã‹ãæ¸©æ³‰ã«å…¥ã‚ŠãŸã„ï¼‰

ã¾ãŸè‰²ã€…ã¨å„å›½ã‚’å›ã‚‹ä¸­ã§ã€æ—¥æœ¬ä¼æ¥­ã£ã¦ã‚¢ã‚¸ã‚¢åœã‚„ä»–ã®å›½ã«ã‚‚ã‹ãªã‚Šé€²å‡ºã—ã¦ã‚‹ã‚“ã ãªãã¨å®Ÿæ„Ÿã—ã¾ã—ãŸã€‚ï¼ˆãã‚Šã‚ƒãã†ï¼‰

ãã‚“ãªã“ã‚“ãªã§æ—¥æœ¬æ ªã«èˆˆå‘³ã‚’æŒã¡
æ˜¨å¹´ã«[ã‚ãŒæŠ•è³‡è¡“](https://amzn.to/3IEVRkq)ã‚’å‚è€ƒã«ã•ã›ã¦ã„ãŸã ããªãŒã‚‰å®Ÿè·µã—å§‹ã‚ã¾ã—ãŸã€‚ï¼ˆã¾ã åˆã‚ã¦ä¸€å¹´ç›®ãªã®ã§æˆç¸¾ã¯ã‚ã‹ã‚Šã¾ã›ã‚“ã€‚ã€‚ã€‚ãŒã€ãƒã‚¤ãƒŠã‚¹ã¯ç„¡ã—ï¼‰

è‡ªåˆ†ã§ãƒãƒ•ã‚§ãƒƒãƒˆã‚³ãƒ¼ãƒ‰ã‚„ Claude yfinance mcp ãªã©ã‚’åˆ©ç”¨ã—ãªãŒã‚‰ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã¿ã¾ã—ãŸãŒã€æ¯å›æ±ºç®—ãŒå‡ºã‚‹ãŸã³ã«æ‰‹å‹•ã¨ãƒãƒ£ãƒƒãƒˆç›¸æ‰‹ã«ã‚ã‚‹ã®ã‚‚ä½•ã‹ãªãã€‚ã¨æ€ã„ã¾ã—ã¦ã€‚

ã˜ã‚ƒã‚è‡ªå‹•åé›†ã¨ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ã‚¢ãƒ—ãƒªä½œã£ã¦ã¿ã‚ˆã†(vibe coding)

ãã‚“ãªãƒãƒªã‹ã‚‰ã€**æ—¥æœ¬æ ªå…¨éŠ˜æŸ„ã‚’è‡ªå‹•åé›†ãƒ»ç°¡æ˜“ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã‚‹ Web ã‚¢ãƒ—ãƒª**ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚

ã“ã®è¨˜äº‹ã§ã¯ã€ç²—æ–¹ã®å·¥ç¨‹ã¨å®Ÿéš›ã«ãƒ­ãƒ¼ã‚«ãƒ«ã§ã“ã‚Œã‚’è©¦ã™æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚
ç´°ã‹ã„ã‚³ãƒ¼ãƒ‰ãªã©ã¯ãã“ã¾ã§æœŸå¾…ã—ãªã„ã§ãã ã•ã„ ğŸ˜…

## ä½œã£ãŸã‚‚ã®

### ğŸ“Š [yfinance-jp-screener](https://github.com/testkun08080/yfinance-jp-screener)

![ã‚µãƒ³ãƒ—ãƒ«](/images/python-yfinance-4c4331412bc50f/img_sample.png)
_æ¤œç´¢éƒ¨åˆ†_
![æ¤œç´¢çµæœ](/images/python-yfinance-4c4331412bc50f/search_result.png)
_æ¤œç´¢çµæœ(ä¼æ¥­åã¯ã“ã“ã§ã¯ä¼ã›ã¦ãŠãã¾ã™)_

**ä¸»ãªæ©Ÿèƒ½:**

- ğŸ“ˆ JPX å…¬å¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç´„ 3,795 éŠ˜æŸ„ã‚’è‡ªå‹•å–å¾—
- ğŸ” è²¡å‹™æŒ‡æ¨™ã«ã‚ˆã‚‹é«˜é€Ÿã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
- ğŸ“Š PBRã€ROEã€è‡ªå·±è³‡æœ¬æ¯”ç‡ãªã©ã®æŒ‡æ¨™å¯è¦–åŒ–
- âš™ï¸ GitHub Actions ã«ã‚ˆã‚‹è‡ªå‹•ãƒ‡ãƒ¼ã‚¿åé›†
- ğŸ³ Docker ç°¡å˜ãƒ‡ãƒ—ãƒ­ã‚¤

# ã€Œã‚ãŒæŠ•è³‡è¡“ã€ã¨ã®å‡ºä¼šã„

[ã‚ãŒæŠ•è³‡è¡“](https://amzn.to/3IEVRkq)ã§ã¯ã€**ã‚·ãƒ³ãƒ—ãƒ«ãªæŒ‡æ¨™**ã§å‰²å®‰æ ªã‚’è¦‹ã¤ã‘ã‚‹æ‰‹æ³•ãŒç´¹ä»‹ã•ã‚Œã¦ã„ã¾ã™ï¼š

- **æ™‚ä¾¡ç·é¡**: 500 å„„ä»¥ä¸‹
- **PBR**: 1 å€ä»¥ä¸‹
- **PER**: 10 å€ä»¥ä¸‹
- **ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥**: ï¼ˆæµå‹•è³‡ç”£ï¼‹æŠ•è³‡æœ‰ä¾¡è¨¼åˆ¸ Ã—70ï¼…ï¼‰ï¼è² å‚µ
- **ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¯”ç‡**ã€€ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥/æ™‚ä¾¡ç·é¡

ã“ã‚Œã‚‰ã®æŒ‡æ¨™ã‚’**è‡ªå‹•ã§å–å¾—ãƒ»åˆ†æ**ã§ãã‚Œã°ã€ã²ã¨ã¾ãšã€Œã‚ãŒæŠ•è³‡è¡“ã€ã«ç²—æ–¹æ²¿ã£ãŸã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãŒã§ãã‚‹ã¨æ€ã„ã¾ã™ã€‚
ãªã®ã§ã€ã“ã‚Œã‚‰ã®åŸºæœ¬çš„ãªã‚‚ã®ã«åŠ ãˆã¦ã€ä»¥ä¸‹ã®ã‚‚ã®ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡ºæ¥ã‚‹ã‚ˆã†ã«ã—ã¦ã„ãã¾ã™ã€‚

### å®Ÿè£…æ¸ˆã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é …ç›®

#### ğŸ“‹ åŸºæœ¬ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼

![search1](/images/python-yfinance-4c4331412bc50f/img_search1.png)

- **ä¼šç¤¾åæ¤œç´¢** - ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
- **éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰æ¤œç´¢** - ã‚³ãƒ¼ãƒ‰æ¤œç´¢
- **æ™‚ä¾¡ç·é¡** -
- **æ¥­ç¨®** - è¤‡æ•°é¸æŠå¯èƒ½ï¼ˆãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ï¼‰
- **å„ªå…ˆå¸‚å ´** - ãƒ—ãƒ©ã‚¤ãƒ /ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰/ã‚°ãƒ­ãƒ¼ã‚¹ï¼ˆè¤‡æ•°é¸æŠï¼‰
- **éƒ½é“åºœçœŒ** - æœ¬ç¤¾æ‰€åœ¨åœ°ã«ã‚ˆã‚‹çµã‚Šè¾¼ã¿ï¼ˆè¤‡æ•°é¸æŠï¼‰

#### ğŸ“Š ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æŒ‡æ¨™

![search2](/images/python-yfinance-4c4331412bc50f/img_search2.png)

- **PBRï¼ˆæ ªä¾¡ç´”è³‡ç”£å€ç‡ï¼‰**
- **ROEï¼ˆè‡ªå·±è³‡æœ¬åˆ©ç›Šç‡ï¼‰**
- **è‡ªå·±è³‡æœ¬æ¯”ç‡**
- **PER(ä¼šäºˆ)ï¼ˆäºˆæƒ³æ ªä¾¡åç›Šç‡ï¼‰**

#### ğŸ’¹ æ¥­ç¸¾ãƒ»åç›Šæ€§æŒ‡æ¨™

![search3](/images/python-yfinance-4c4331412bc50f/img_search3.png)

- **å£²ä¸Šé«˜**
- **å–¶æ¥­åˆ©ç›Š**
- **å–¶æ¥­åˆ©ç›Šç‡**
- **å½“æœŸç´”åˆ©ç›Š**
- **ç´”åˆ©ç›Šç‡**

#### ğŸ›ï¸ ãƒãƒ©ãƒ³ã‚¹ã‚·ãƒ¼ãƒˆæŒ‡æ¨™

![search4](/images/python-yfinance-4c4331412bc50f/img_search4.png)

- **è² å‚µ**
- **æµå‹•è² å‚µ**
- **æµå‹•è³‡ç”£**
- **ç·è² å‚µ**
- **æŠ•è³‡æœ‰ä¾¡è¨¼åˆ¸**

#### ğŸ’° ã‚­ãƒ£ãƒƒã‚·ãƒ¥é–¢é€£æŒ‡æ¨™

![search5](/images/python-yfinance-4c4331412bc50f/img_search5.png)

- **ç¾é‡‘åŠã³ç¾é‡‘åŒç­‰ç‰©**
- **ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥**
- **ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¯”ç‡**

# æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ (JPXå…¬å¼ + Yahoo Finance)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                               â”‚
         â†“                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub Actions    â”‚        â”‚ ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ          â”‚
â”‚  (CI/CDè‡ªå‹•åé›†)    â”‚        â”‚ (Docker Compose).   â”‚
â”‚                    â”‚        â”‚                     â”‚
â”‚  Part 1-4          â”‚        â”‚  Python Service     â”‚
â”‚  â†’ CSV Combine     â”‚        â”‚  â†’ CSVç”Ÿæˆ           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                               â”‚
         â†“                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CSV ãƒ•ã‚¡ã‚¤ãƒ« (Export/)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         React ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ (Docker/Local)      ã€€â”‚
â”‚  CSV Drag & Drop â†’ æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ â†’ çµæœè¡¨ç¤º        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

:::details è©³ç´°

```mermaid
graph TB
    subgraph "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹"
        JPX[JPXå…¬å¼ãƒ‡ãƒ¼ã‚¿<br/>ç´„3,795éŠ˜æŸ„]
        Yahoo[Yahoo Finance API<br/>yfinance]
    end

    subgraph "æ–¹æ³•1: GitHub Actions - è‡ªå‹•åé›†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"
        StockList[Stock List Update<br/>æ ªå¼ãƒªã‚¹ãƒˆå–å¾—]
        Part1[Sequential Part 1<br/>stocks_1.json<br/>1-1000ç¤¾]
        Part2[Sequential Part 2<br/>stocks_2.json<br/>1001-2000ç¤¾]
        Part3[Sequential Part 3<br/>stocks_3.json<br/>2001-3000ç¤¾]
        Part4[Sequential Part 4<br/>stocks_4.json<br/>3001-3795ç¤¾]
        Combine[CSV Combine<br/>ãƒ‡ãƒ¼ã‚¿çµåˆ]
        CSV_GA[CSVå‡ºåŠ›<br/>Export/]
    end

    subgraph "æ–¹æ³•2: ãƒ­ãƒ¼ã‚«ãƒ« Docker Compose"
        subgraph "Python Service"
            DataProcess[ãƒ‡ãƒ¼ã‚¿å‡¦ç†<br/>sumalize.py]
            CSV_Local[CSVç”Ÿæˆ<br/>Export/]
        end
    end

    subgraph "ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ¸ˆã¿CSV"
        CSVFiles[(CSV ãƒ•ã‚¡ã‚¤ãƒ«<br/>Export/<br/>YYYYMMDD_combined.csv)]
    end

    subgraph "å®Œæˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ (Docker/Local)"
        subgraph "React Webã‚¢ãƒ—ãƒª"
            React[React 19 + TypeScript<br/>Vite]
            Nginx[nginx<br/>é™çš„é…ä¿¡]
        end

        subgraph "ãƒ–ãƒ©ã‚¦ã‚¶ UI"
            UI[Webã‚¢ãƒ—ãƒªèµ·å‹•<br/>localhost:8080/5173]
            Upload[CSV DnD<br/>ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½]
            Table[ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«<br/>æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿]
            Analysis[è²¡å‹™åˆ†æ<br/>PBR/ROE/è‡ªå·±è³‡æœ¬æ¯”ç‡]
        end
    end

    %% GitHub Actions ãƒ•ãƒ­ãƒ¼
    JPX -->|Excelâ†’JSONå¤‰æ›| StockList
    StockList -->|è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼| Part1
    Part1 -->|è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼| Part2
    Part2 -->|è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼| Part3
    Part3 -->|è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼| Part4
    Part4 -->|è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼| Combine

    Yahoo -->|APIå–å¾—| Part1
    Yahoo -->|APIå–å¾—| Part2
    Yahoo -->|APIå–å¾—| Part3
    Yahoo -->|APIå–å¾—| Part4

    Combine -->|CSVå‡ºåŠ›| CSV_GA
    CSV_GA -->|ä¿å­˜| CSVFiles

    %% ãƒ­ãƒ¼ã‚«ãƒ« Docker ãƒ•ãƒ­ãƒ¼
    JPX -->|JSONå–å¾—| DataProcess
    Yahoo -->|APIå–å¾—| DataProcess
    DataProcess -->|CSVç”Ÿæˆ| CSV_Local
    CSV_Local -->|ä¿å­˜| CSVFiles

    %% ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•ãƒ•ãƒ­ãƒ¼
    React -->|ãƒ“ãƒ«ãƒ‰| Nginx
    Nginx -->|HTTPé…ä¿¡| UI

    %% ãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œãƒ•ãƒ­ãƒ¼ (CSV â†’ ã‚¢ãƒ—ãƒª)
    CSVFiles -.->|ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‰‹å‹•ã§<br/>Drag & Drop| Upload
    Upload -->|CSVãƒ‘ãƒ¼ã‚¹| Table
    Table --> Analysis

    style JPX fill:#e1f5ff
    style Yahoo fill:#e1f5ff
    style StockList fill:#fff4e6
    style Part1 fill:#f3e5f5
    style Part2 fill:#f3e5f5
    style Part3 fill:#f3e5f5
    style Part4 fill:#f3e5f5
    style Combine fill:#fff4e6
    style CSV_GA fill:#e8f5e9
    style DataProcess fill:#e8f5e9
    style CSV_Local fill:#e8f5e9
    style CSVFiles fill:#fce4ec
    style Upload fill:#ffe0b2
    style React fill:#e3f2fd
    style Nginx fill:#e3f2fd
    style UI fill:#f1f8e9
    style Table fill:#fff9c4
    style Analysis fill:#fff9c4
```

:::

## ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆPythonï¼‰

- **Python 3.11+**
- **yfinance**

## ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ï¼ˆReactï¼‰

- **React 19**
- **TypeScript**
- **Vite**
- **Tailwind CSS + DaisyUI**

## ã‚¤ãƒ³ãƒ•ãƒ©

- **GitHub Actions**
- **Docker Compose**

# é–‹ç™ºã®ãƒã‚¤ãƒ³ãƒˆ

## 1. ãƒ‡ãƒ¼ã‚¿åé›†ã®è‡ªå‹•åŒ–

### èª²é¡Œ: yfinance API ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™

ç´„ 3,795 ç¤¾ã®ãƒ‡ãƒ¼ã‚¿ã‚’ Github Actions ã§ä¸€åº¦ã«å–å¾—ã™ã‚‹ã¨ã€API ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒç™ºç”Ÿã—ã¾ã™ã€‚
ï¼ˆç®¡ç†ä¸Šã‚‚åˆ†ã‘ãŸã‹ã£ãŸã¨ã„ã†æ„å›³ã‚‚ã‚ã‚Šã¾ã™ã€‚ï¼‰

### åˆ†å‰²å‡¦ç†

GitHub Actions ã§**4 æ®µéšã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**ã‚’æ§‹ç¯‰ã—ã€è‡ªå‹•é€£æºã•ã›ã¾ã—ãŸã€‚

```yaml
# Part 1 â†’ Part 2 â†’ Part 3 â†’ Part 4 â†’ CSVçµåˆ
Sequential Stock Fetch - Part 1 (stocks_1.json: 1,000ç¤¾)
  â†“ è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼
Sequential Stock Fetch - Part 2 (stocks_2.json: 1,000ç¤¾)
  â†“ è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼
Sequential Stock Fetch - Part 3 (stocks_3.json: 1,000ç¤¾)
  â†“ è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼
Sequential Stock Fetch - Part 4 (stocks_4.json: 795ç¤¾)
  â†“ è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼
CSV Combine & Export (å…¨ãƒ‡ãƒ¼ã‚¿çµåˆ)
```

### å®Ÿè£…ã‚³ãƒ¼ãƒ‰ï¼ˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é€£æºéƒ¨åˆ†ï¼‰

```yaml
name: ğŸ“Š Sequential Stock Fetch - Part 1

permissions:
  contents: write
  actions: write

on:
  workflow_dispatch:
    inputs:
      reason:
        description: "é–‹å§‹ç†ç”±ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"
        required: false
        default: "Sequential stock data collection - Part 1"
        type: string

jobs:
  fetch-stocks-1:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    permissions:
      contents: write

    outputs:
      success: ${{ steps.process.outcome == 'success' }}

    steps:
      - name: ğŸ”„ Checkout repository
        uses: actions/checkout@v4

      - name: ğŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          cache: "pip"

      - name: ğŸ“¦ Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r stock_list/requirements.txt

      - name: ğŸ“‹ Show process info
        run: |
          echo "ğŸš€ Sequential Stock Fetch - Part 1/4"
          echo "Processing file: stocks_1.json"
          echo "Reason: ${{ github.event.inputs.reason }}"
          echo "Timestamp: $(date)"
          echo "Working directory: $(pwd)"
          ls -la stock_list/

      - name: ğŸƒ Process stocks_1.json
        id: process
        working-directory: ./stock_list
        run: |
          echo "ğŸš€ Starting stock data collection for stocks_1.json..."
          echo "Timestamp: $(date)"

          python sumalize.py "stocks_1.json"

          echo "âœ… Part 1 completed successfully"
          echo "ğŸ“„ Generated files in Export directory:"
          ls -la Export/ 2>/dev/null || echo "No files in Export directory"

      - name: Git config and pull
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git pull origin main --rebase || true

      - name: ğŸ’¾ Commit changes - Part 1
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "ğŸ“Š Sequential Stock Fetch - Part 1/4 å®Œäº† ($(date +'%Yå¹´%mæœˆ%dæ—¥ %H:%M'))"
          push_options: --force

  trigger-part-2:
    needs: fetch-stocks-1
    runs-on: ubuntu-latest
    if: needs.fetch-stocks-1.outputs.success == 'true'

    steps:
      - name: ğŸ”„ Checkout repository
        uses: actions/checkout@v4

      - name: ğŸš€ Trigger Part 2
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const result = await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'stock-fetch-sequential-2.yml',
              ref: 'main',
              inputs: {
                reason: 'Auto-triggered by Part 1 completion'
              }
            });

            console.log('âœ… Part 2 triggered successfully');
            console.log('Response status:', result.status);
```

## 2. ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®åŠ¹ç‡åŒ–

### JPX å…¬å¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾—

JPXï¼ˆæ—¥æœ¬å–å¼•æ‰€ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰ã®å…¬å¼ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‹ã‚‰ã€
ä¸Šå ´ä¼æ¥­ã®æœ€æ–°æ ªå¼ãƒªã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€JSON å½¢å¼ã§ä¿å­˜ã—ã¾ã™ã€‚

```python
import requests
import pandas as pd
import xlrd
from openpyxl import Workbook
import json
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()],
)
logger = logging.getLogger(__name__)

# ãƒ•ã‚¡ã‚¤ãƒ«ã®URL
url = "https://www.jpx.co.jp/markets/statistics-equities/misc/tvdivq0000001vg2-att/data_j.xls"

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
response = requests.get(url)

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
xls_file = "tickers.xls"
with open(xls_file, "wb") as f:
    f.write(response.content)

# .xlsãƒ•ã‚¡ã‚¤ãƒ«ã‚’ .xlsx ã«å¤‰æ›
xlsx_file = "converted.xlsx"
workbook_xls = xlrd.open_workbook(xls_file)
sheet_xls = workbook_xls.sheet_by_index(0)

workbook_xlsx = Workbook()
sheet_xlsx = workbook_xlsx.active

# ãƒ‡ãƒ¼ã‚¿ã‚’ .xls ã‹ã‚‰ .xlsx ã«æ›¸ãè¾¼ã‚€
for row in range(sheet_xls.nrows):
    for col in range(sheet_xls.ncols):
        sheet_xlsx.cell(row=row + 1, column=col + 1).value = sheet_xls.cell_value(row, col)

# .xlsx ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
workbook_xlsx.save(xlsx_file)

# å¤‰æ›ã•ã‚ŒãŸ .xlsx ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
data = pd.read_excel(xlsx_file)

# ORæ¡ä»¶ã‚’ä½¿ç”¨ã—ã¦æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹è¡Œã‚’æŠ½å‡º
condition = (
    (data["å¸‚å ´ãƒ»å•†å“åŒºåˆ†"] == "ãƒ—ãƒ©ã‚¤ãƒ ï¼ˆå†…å›½æ ªå¼ï¼‰")
    | (data["å¸‚å ´ãƒ»å•†å“åŒºåˆ†"] == "ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ï¼ˆå†…å›½æ ªå¼ï¼‰")
    | (data["å¸‚å ´ãƒ»å•†å“åŒºåˆ†"] == "ã‚°ãƒ­ãƒ¼ã‚¹ï¼ˆå†…å›½æ ªå¼ï¼‰")
)

filtered_df = data[condition]

# å¿…è¦ãªåˆ—ã ã‘ã‚’æŠ½å‡º
selected_df = filtered_df[["ã‚³ãƒ¼ãƒ‰", "éŠ˜æŸ„å", "å¸‚å ´ãƒ»å•†å“åŒºåˆ†", "33æ¥­ç¨®åŒºåˆ†"]]

# DataFrame ã‚’ JSON å½¢å¼ï¼ˆãƒªã‚¹ãƒˆã®è¾æ›¸å½¢å¼ï¼‰ã«å¤‰æ›
json_list = selected_df.to_dict(orient="records")

# JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
with open("stocks_all.json", "w", encoding="utf-8") as f:
    json.dump(json_list, f, ensure_ascii=False, indent=2)

logger.info("JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: stocks_all.json")

```

### ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ã‚’ json ã¸ä¿å­˜ã—ç›´ã™ï¼ˆåˆ†å‰²ã—ãŸã„ãŸã‚ï¼‰

stocks_all.json ã‚’ XXXX ç¤¾ãšã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
import json
import math
import argparse
import sys
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()],
)
logger = logging.getLogger(__name__)


def split_stocks_json(input_file="stocks_all.json", chunk_size=1000):
    """
    stocks_all.jsonã‚’æŒ‡å®šã•ã‚ŒãŸã‚µã‚¤ã‚ºã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²

    Args:
        input_file (str): å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«å
        chunk_size (int): 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®ä¼æ¥­æ•°
    """
    try:
        # å…ƒã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        with open(input_file, "r", encoding="utf-8") as f:
            stock_data = json.load(f)

        total_companies = len(stock_data)
        total_files = math.ceil(total_companies / chunk_size)

        logger.info(f"ç·ä¼æ¥­æ•°: {total_companies}ç¤¾")
        logger.info(f"åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
        logger.info(f"1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Š: æœ€å¤§{chunk_size}ç¤¾")
        logger.info("-" * 50)

        # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦ä¿å­˜
        for i in range(total_files):
            start_idx = i * chunk_size
            end_idx = min((i + 1) * chunk_size, total_companies)
            chunk_data = stock_data[start_idx:end_idx]

            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆstocks_1.json, stocks_2.json, ...ï¼‰
            output_filename = f"stocks_{i + 1}.json"

            # JSONå½¢å¼ã§ä¿å­˜
            with open(output_filename, "w", encoding="utf-8") as f:
                json.dump(chunk_data, f, ensure_ascii=False, indent=2)

            logger.info(
                f"âœ… {output_filename}: {len(chunk_data)}ç¤¾ (#{start_idx + 1}-#{end_idx})"
            )

        logger.info("-" * 50)
        logger.info(f"åˆ†å‰²å®Œäº†: {total_files}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ")

        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ã‚’è¡¨ç¤º
        logger.info("\nä½œæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        for i in range(total_files):
            filename = f"stocks_{i + 1}.json"
            with open(filename, "r", encoding="utf-8") as f:
                data = json.load(f)
            logger.info(f"  {filename}: {len(data)}ç¤¾")

    except FileNotFoundError:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {input_file}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    except json.JSONDecodeError:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {input_file}ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="æ—¥æœ¬æ ªãƒªã‚¹ãƒˆJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã•ã‚ŒãŸã‚µã‚¤ã‚ºã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã™",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  python split_stocks.py                           # stocks_all.jsonã‚’1000ç¤¾ãšã¤ã«åˆ†å‰²
  python split_stocks.py -i stocks_all.json       # stocks_all.jsonã‚’1000ç¤¾ãšã¤ã«åˆ†å‰²
  python split_stocks.py -i data.json -s 500      # data.jsonã‚’500ç¤¾ãšã¤ã«åˆ†å‰²
  python split_stocks.py --input stocks_all.json --size 2000  # 2000ç¤¾ãšã¤ã«åˆ†å‰²
        """,
    )

    parser.add_argument(
        "-i",
        "--input",
        default="stocks_all.json",
        help="å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«å (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: stocks_all.json)",
    )

    parser.add_argument(
        "-s",
        "--size",
        type=int,
        default=1000,
        help="1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®ä¼æ¥­æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000)",
    )

    parser.add_argument("-v", "--verbose", action="store_true", help="è©³ç´°ãªå‡ºåŠ›ã‚’è¡¨ç¤º")

    args = parser.parse_args()

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if args.size <= 0:
        logger.error("âŒ ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¯æ­£ã®æ•´æ•°ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        sys.exit(1)

    logger.info("=" * 60)
    logger.info("ğŸ“Š stocks_all.jsonåˆ†å‰²ãƒ„ãƒ¼ãƒ«")
    logger.info("=" * 60)
    logger.info(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {args.input}")
    logger.info(f"ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {args.size}ç¤¾")
    if args.verbose:
        logger.info("è©³ç´°ãƒ¢ãƒ¼ãƒ‰: ON")
    logger.info("=" * 60)

    split_stocks_json(input_file=args.input, chunk_size=args.size)

```

#### yfinance ã§ã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—

ãƒ¡ã‚¤ãƒ³å‡¦ç†ã¯ã€ã‚·ãƒ³ãƒ—ãƒ«ã«èª­ã¿è¾¼ã‚“ã  json é‡ä¸­ã‹ã‚‰ãƒ†ã‚£ãƒƒã‚«ãƒ¼çµã‚‹ã”ã¨ã«å›ã‚‹ã ã‘ã§ã™ã€‚

```python
def main(json_filename="stocks_sample.json"):
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†

    Args:
        json_filename (str): å‡¦ç†å¯¾è±¡ã®JSONãƒ•ã‚¡ã‚¤ãƒ«å
    """
    overall_start_time = time.time()
    overall_start_datetime = datetime.now()

    logger.info("=" * 80)
    logger.info(f"æ—¥æœ¬æ ªè²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹ - é–‹å§‹æ™‚åˆ»: {overall_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {json_filename}")
    logger.info("=" * 80)

    # æŒ‡å®šã•ã‚ŒãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    try:
        with open(json_filename, "r", encoding="utf-8") as f:
            stock_list = json.load(f)
        logger.info(f"{json_filename}ã‹ã‚‰{len(stock_list)}ç¤¾ã®éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    except FileNotFoundError:
        logger.error(f"âŒ {json_filename}ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    except json.JSONDecodeError:
        logger.error(f"âŒ {json_filename}ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
        return None

    logger.info("=" * 60)
    logger.info("æ—¥æœ¬æ ªè²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
    logger.info("=" * 60)

    results = []

    for i, stock in enumerate(stock_list, 1):
        logger.info(f"\n[{i}/{len(stock_list)}]")
        result = get_stock_data(stock)

        if result:
            results.append(result)

        # APIåˆ¶é™å›é¿ã®ãŸã‚å°‘ã—å¾…æ©Ÿ
        if i < len(stock_list):
            time.sleep(2)

    # çµæœã‚’DataFrameã«å¤‰æ›
    if results:
        df = pd.DataFrame(results)

        # åˆ—ã®é †åºã‚’æŒ‡å®š
        columns_order = [
            "ä¼šç¤¾å",
            "éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰",
            "æ¥­ç¨®",
            "å„ªå…ˆå¸‚å ´",
            "æ±ºç®—æœˆ",
            # "ä¼šè¨ˆåŸºæº–",  # ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
            "éƒ½é“åºœçœŒ",
            "æ™‚ä¾¡ç·é¡",
            "PBR",
            "å£²ä¸Šé«˜",
            "å–¶æ¥­åˆ©ç›Š",
            "å–¶æ¥­åˆ©ç›Šç‡",
            "å½“æœŸç´”åˆ©ç›Š",
            "ç´”åˆ©ç›Šç‡",
            "ROE",
            "è‡ªå·±è³‡æœ¬æ¯”ç‡",
            "PER(ä¼šäºˆ)",
            "è² å‚µ",
            "æµå‹•è² å‚µ",
            "æµå‹•è³‡ç”£",
            "ç·è² å‚µ",
            "ç¾é‡‘åŠã³ç¾é‡‘åŒç­‰ç‰©",
            "æŠ•è³‡æœ‰ä¾¡è¨¼åˆ¸",
            "ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥",
            "ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¯”ç‡",
        ]

        df = df.reindex(columns=columns_order)

        overall_end_time = time.time()
        overall_end_datetime = datetime.now()
        overall_duration = overall_end_time - overall_start_time

        # çµæœã‚’è¡¨ç¤º
        logger.info("\n" + "=" * 60)
        logger.info("å–å¾—çµæœã‚µãƒãƒªãƒ¼")
        logger.info("=" * 60)
        logger.info(f"å–å¾—æˆåŠŸ: {len(results)}ç¤¾")
        logger.info(f"å–å¾—å¤±æ•—: {len(stock_list) - len(results)}ç¤¾")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆExport ãƒ•ã‚©ãƒ«ãƒ€ã«ç›´æ¥ä¿å­˜ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = json_filename.replace(".json", "").replace("stocks_", "")

        filename = f"Export/japanese_stocks_data_{base_name}_{timestamp}.csv"
        df.to_csv(filename, index=False, encoding="utf-8-sig")
        logger.info(f"\nãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {filename}")

        # ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’è¡¨ç¤º
        logger.info("\nå–å¾—ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€åˆã®3åˆ—ï¼‰:")
        logger.info(f"\n{df[['ä¼šç¤¾å', 'éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰', 'æ™‚ä¾¡ç·é¡', 'PBR', 'ROE']].head()}")

        # å…¨ä½“ã®å®Ÿè¡Œæ™‚é–“ã‚’ãƒ­ã‚°å‡ºåŠ›
        logger.info("=" * 80)
        logger.info("æ—¥æœ¬æ ªè²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†")
        logger.info(f"é–‹å§‹æ™‚åˆ»: {overall_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"çµ‚äº†æ™‚åˆ»: {overall_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"ç·å®Ÿè¡Œæ™‚é–“: {format_duration(overall_duration)}")
        logger.info(
            f"å‡¦ç†çµæœ: æˆåŠŸ {len(results)}ç¤¾ / å¤±æ•— {len(stock_list) - len(results)}ç¤¾ / åˆè¨ˆ {len(stock_list)}ç¤¾"
        )
        logger.info(f"å¹³å‡å‡¦ç†æ™‚é–“: {format_duration(overall_duration / len(stock_list))}ï¼ˆ1ç¤¾ã‚ãŸã‚Šï¼‰")
        logger.info(f"ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
        logger.info("=" * 80)

        return df
    else:
        overall_end_time = time.time()
        overall_end_datetime = datetime.now()
        overall_duration = overall_end_time - overall_start_time

        logger.error("\nâŒ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        logger.error("=" * 80)
        logger.error("æ—¥æœ¬æ ªè²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ—ãƒ­ã‚»ã‚¹å¤±æ•—")
        logger.error(f"é–‹å§‹æ™‚åˆ»: {overall_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.error(f"çµ‚äº†æ™‚åˆ»: {overall_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.error(f"ç·å®Ÿè¡Œæ™‚é–“: {format_duration(overall_duration)}")
        logger.error("ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        logger.error("=" * 80)
        return None


```

å€‹åˆ¥éŠ˜æŸ„ã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ã„ãã¾ã™ã€‚
ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ã”ã¨ã«ã»ã—ã„æƒ…å ±ã‚’æ—¥æœ¬èªã«ç›´ã—ãªãŒã‚‰ã€ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã£ã¦è¿”ã—ã¾ã™ã€‚

```python
def get_stock_data(stock_info):
    code = stock_info["ã‚³ãƒ¼ãƒ‰"]
    ticker_symbol = format_ticker(code)

    start_time = time.time()
    start_datetime = datetime.now()

    logger.info(f"å–å¾—ä¸­: {stock_info['éŠ˜æŸ„å']} ({ticker_symbol})")
    logger.debug(
        f"ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: {stock_info['éŠ˜æŸ„å']} ({ticker_symbol}) - é–‹å§‹æ™‚åˆ»: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}"
    )

    try:
        # yfinanceã§ãƒ†ã‚£ãƒƒã‚«ãƒ¼ä½œæˆ
        ticker = yf.Ticker(ticker_symbol)

        # åŸºæœ¬æƒ…å ±å–å¾—
        info = ticker.info
        if not info:
            logger.warning(f"  âš ï¸ åŸºæœ¬æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {ticker_symbol}")
            return None

        # æ™‚é–“ã‚’ç½®ã„ã¦APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å›é¿
        time.sleep(0.5)

        # è²¡å‹™è«¸è¡¨ãƒ‡ãƒ¼ã‚¿å–å¾—
        try:
            financials = ticker.financials
            balance_sheet = ticker.balance_sheet
        except Exception as e:
            logger.warning(f"  âš ï¸ è²¡å‹™è«¸è¡¨å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            financials = pd.DataFrame()
            balance_sheet = pd.DataFrame()

        # æ±ºç®—æœˆã‚’å–å¾—ï¼ˆãƒãƒ©ãƒ³ã‚¹ã‚·ãƒ¼ãƒˆã®æœ€æ–°æœŸã‹ã‚‰ï¼‰
        settlement_period = None
        if not balance_sheet.empty:
            cols = balance_sheet.columns.tolist()
            if cols:
                # æœ€æ–°æ±ºç®—æœŸã‹ã‚‰æ—¥ä»˜éƒ¨åˆ†ã®ã¿æŠ½å‡ºï¼ˆä¾‹ï¼š2025-03-31ï¼‰
                latest_period = cols[0]
                if hasattr(latest_period, "strftime"):
                    # datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã€æ—¥ä»˜éƒ¨åˆ†ã®ã¿å–å¾—
                    settlement_period = latest_period.strftime("%Y-%m-%d")
                else:
                    # æ–‡å­—åˆ—ã®å ´åˆã€æ™‚é–“éƒ¨åˆ†ã‚’å‰Šé™¤
                    settlement_period = str(latest_period).split(" ")[0]

        # PER(ä¼šäºˆ)ã®ãƒ‡ãƒãƒƒã‚°
        forward_pe = info.get("forwardPE", None)

        # ãƒ‡ãƒ¼ã‚¿åé›†
        result = {
            "ä¼šç¤¾å": stock_info["éŠ˜æŸ„å"] or safe_get_value(info, "longName") or safe_get_value(info, "shortName"),
            "éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰": code,
            "æ¥­ç¨®": stock_info.get("33æ¥­ç¨®åŒºåˆ†") or safe_get_value(info, "industry") or safe_get_value(info, "sector"),
            "å„ªå…ˆå¸‚å ´": stock_info.get("å¸‚å ´ãƒ»å•†å“åŒºåˆ†", ""),
            "æ±ºç®—æœˆ": settlement_period,
            # "ä¼šè¨ˆåŸºæº–": None,  # yfinanceã§ã¯è©³ç´°ä¸æ˜ - ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
            "éƒ½é“åºœçœŒ": get_prefecture_from_zip(safe_get_value(info, "zip")) or None,
            "æ™‚ä¾¡ç·é¡": safe_get_value(info, "marketCap"),
            "PBR": safe_get_value(info, "priceToBook"),
            "PER(ä¼šäºˆ)": forward_pe,
            "ROE": safe_get_value(info, "returnOnEquity"),
            "å–¶æ¥­åˆ©ç›Šç‡": safe_get_value(info, "operatingMargins"),
            "ç´”åˆ©ç›Šç‡": safe_get_value(info, "profitMargins"),
        }

        # è²¡å‹™è«¸è¡¨ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
        if not financials.empty:
            result["å£²ä¸Šé«˜"] = safe_get_financial_data(ticker, "financials", "Total Revenue")
            result["å–¶æ¥­åˆ©ç›Š"] = safe_get_financial_data(ticker, "financials", "Operating Income")
            result["å½“æœŸç´”åˆ©ç›Š"] = safe_get_financial_data(ticker, "financials", "Net Income")
        else:
            result.update({"å£²ä¸Šé«˜": None, "å–¶æ¥­åˆ©ç›Š": None, "å½“æœŸç´”åˆ©ç›Š": None})

        if not balance_sheet.empty:
            # ãƒãƒ©ãƒ³ã‚¹ã‚·ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆtest.csvã®é …ç›®åã«åŸºã¥ãã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
            total_liabilities = safe_get_financial_data(
                ticker,
                "balance_sheet",
                "Total Liabilities Net Minority Interest",
                fallback_items=["Total Liab"],
            )
            current_liabilities = safe_get_financial_data(
                ticker,
                "balance_sheet",
                "Current Liabilities",
                fallback_items=["Total Current Liabilities"],
            )
            current_assets = safe_get_financial_data(
                ticker,
                "balance_sheet",
                "Current Assets",
                fallback_items=["Total Current Assets"],
            )
            total_equity = safe_get_financial_data(
                ticker,
                "balance_sheet",
                "Stockholders Equity",
                fallback_items=["Total Stockholder Equity"],
            )
            total_assets = safe_get_financial_data(ticker, "balance_sheet", "Total Assets")
            total_debt = safe_get_financial_data(ticker, "balance_sheet", "Total Debt")
            cash_and_equivalents = safe_get_financial_data(
                ticker,
                "balance_sheet",
                "Cash And Cash Equivalents",
                fallback_items=["Cash Cash Equivalents And Short Term Investments"],
            )
            investments = safe_get_financial_data(
                ticker,
                "balance_sheet",
                "Available For Sale Securities",
                fallback_items=[
                    "Short Term Investments",
                    "Investmentin Financial Assets",
                ],
            )

            result.update({
                "è² å‚µ": total_liabilities,
                "æµå‹•è² å‚µ": current_liabilities,
                "æµå‹•è³‡ç”£": current_assets,
                "ç·è² å‚µ": total_debt,
                "ç¾é‡‘åŠã³ç¾é‡‘åŒç­‰ç‰©": cash_and_equivalents,
                "æŠ•è³‡æœ‰ä¾¡è¨¼åˆ¸": investments,
            })

            # è‡ªå·±è³‡æœ¬æ¯”ç‡ã®è¨ˆç®—
            if total_equity and total_assets:
                result["è‡ªå·±è³‡æœ¬æ¯”ç‡"] = total_equity / total_assets
            else:
                result["è‡ªå·±è³‡æœ¬æ¯”ç‡"] = None

            # ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨ˆç®—ï¼ˆæµå‹•è³‡ç”£ + æŠ•è³‡æœ‰ä¾¡è¨¼åˆ¸Ã—70% - è² å‚µï¼‰
            net_cash = calculate_net_cash(current_assets, investments, total_liabilities)
            result["ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥"] = net_cash

            # ãƒ‡ãƒãƒƒã‚°ç”¨: ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨ˆç®—ã®è©³ç´°ã‚’è¡¨ç¤º
            if any(x is not None for x in [current_assets, investments, total_liabilities]):
                inv_70 = (investments * 0.7) if investments is not None else 0
                logger.debug(
                    f"  ğŸ“Š ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨ˆç®—: {current_assets} + {inv_70:.0f} - {total_liabilities} = {net_cash}"
                )

            # ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¯”ç‡ã®è¨ˆç®—
            if net_cash and result["æ™‚ä¾¡ç·é¡"]:
                result["ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¯”ç‡"] = net_cash / result["æ™‚ä¾¡ç·é¡"]
            else:
                result["ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¯”ç‡"] = None
        else:
            result.update({
                "è² å‚µ": None,
                "æµå‹•è² å‚µ": None,
                "æµå‹•è³‡ç”£": None,
                "ç·è² å‚µ": None,
                "ç¾é‡‘åŠã³ç¾é‡‘åŒç­‰ç‰©": None,
                "æŠ•è³‡æœ‰ä¾¡è¨¼åˆ¸": None,
                "è‡ªå·±è³‡æœ¬æ¯”ç‡": None,
                "ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥": None,
                "ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¯”ç‡": None,
            })

        end_time = time.time()
        end_datetime = datetime.now()
        duration = end_time - start_time

        logger.info(f"  âœ… å–å¾—å®Œäº†: {result['ä¼šç¤¾å']}")
        logger.debug(
            f"ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {result['ä¼šç¤¾å']} ({ticker_symbol}) - çµ‚äº†æ™‚åˆ»: {end_datetime.strftime('%Y-%m-%d %H:%M:%S')} - å®Ÿè¡Œæ™‚é–“: {format_duration(duration)}"
        )
        return result

    except Exception as e:
        end_time = time.time()
        end_datetime = datetime.now()
        duration = end_time - start_time

        logger.error(f"  âŒ ã‚¨ãƒ©ãƒ¼: {ticker_symbol} - {e}")
        logger.error(
            f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {stock_info['éŠ˜æŸ„å']} ({ticker_symbol}) - çµ‚äº†æ™‚åˆ»: {end_datetime.strftime('%Y-%m-%d %H:%M:%S')} - å®Ÿè¡Œæ™‚é–“: {format_duration(duration)} - ã‚¨ãƒ©ãƒ¼: {e}"
        )
        return None


```

éƒ½é“åºœçœŒã‚’çŸ¥ã‚ŠãŸã‹ã£ãŸãŸã‚ã€yfinace ã§éƒµä¾¿ç•ªå·ãŒå–ã‚Œã‚‹ã®ã§ã€
éƒµä¾¿ç•ªå·ã‹ã‚‰éƒ½é“åºœçœŒã‚’å–å¾—ã™ã‚‹ã®ã«ã¯ã€

https://github.com/GitHub30/digital-address.php

ã“ã¡ã‚‰ã‚’ä½¿ã‚ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚ï¼ˆã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼ï¼‰

```python
def get_prefecture_from_zip(zip_code):
    """éƒµä¾¿ç•ªå·ã‹ã‚‰éƒ½é“åºœçœŒåã‚’å–å¾—ï¼ˆdigital-address APIä½¿ç”¨ï¼‰

    Args:
        zip_code (str): éƒµä¾¿ç•ªå·ï¼ˆãƒã‚¤ãƒ•ãƒ³ã‚ã‚Š/ãªã—ä¸¡æ–¹å¯¾å¿œï¼‰

    Returns:
        str: éƒ½é“åºœçœŒåï¼ˆä¾‹: "æ±äº¬éƒ½", "å¤§é˜ªåºœ"ï¼‰
        None: å–å¾—å¤±æ•—æ™‚ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãªã—

    Note:
        - digital-address APIã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å–å¾—
        - éƒµä¾¿ç•ªå·ã®å‰å‡¦ç†ï¼ˆãƒã‚¤ãƒ•ãƒ³ãƒ»ç©ºç™½é™¤å»ï¼‰ã‚’è‡ªå‹•å®Ÿè¡Œ
        - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š: 10ç§’
    """
    try:
        if not zip_code:
            return None

        # éƒµä¾¿ç•ªå·ã®å‰å‡¦ç†ï¼ˆãƒã‚¤ãƒ•ãƒ³ã‚„ç©ºç™½ã‚’é™¤å»ï¼‰
        clean_zip = str(zip_code).replace("-", "").replace("âˆ’", "").replace(" ", "").replace("ã€€", "")

        if len(clean_zip) < 7:  # éƒµä¾¿ç•ªå·ã¨ã—ã¦çŸ­ã™ãã‚‹å ´åˆ
            return None

        # digital-address APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        url = f"https://digital-address.app/{clean_zip}"

        response = requests.get(url, timeout=10)
        response.raise_for_status()

        data = response.json()

        if data.get("addresses") and len(data["addresses"]) > 0:
            # addressesã®æœ€åˆã®è¦ç´ ã‹ã‚‰pref_nameã‚’å–å¾—
            address = data["addresses"][0]
            prefecture = address.get("pref_name")
            logger.debug(f"  ğŸ¢ éƒ½é“åºœçœŒ: {prefecture}")
            return prefecture

        return None

    except Exception as e:
        logger.debug(f"    éƒµä¾¿ç•ªå·å¤‰æ›ã‚¨ãƒ©ãƒ¼ ({zip_code}): {e}")
        return None

```

## 3. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®å®Ÿè£…

:::message
ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€23 é …ç›®ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€URL ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é€£æºã€ã‚½ãƒ¼ãƒˆæ©Ÿèƒ½ãªã©ã®æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚
:::

### å‹•çš„ CSV ãƒ‘ãƒ¼ã‚¹ & ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

```typescript
// csvParser.ts
import Papa from "papaparse";

export interface StockData {
  [key: string]: string | number;
}

export const parseCSV = (csvText: string): StockData[] => {
  const result = Papa.parse<StockData>(csvText, {
    header: true,
    dynamicTyping: true,
    skipEmptyLines: true,
    encoding: "UTF-8",
  });

  return result.data;
};

// æ—¥æœ¬èªé‡‘èãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
export const formatValue = (value: any, columnName: string): string => {
  if (value === null || value === undefined) return "N/A";

  // ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸
  if (columnName.includes("ç‡") || columnName.includes("ROE")) {
    return `${(value * 100).toFixed(2)}%`;
  }

  // é‡‘é¡ï¼ˆå„„å††å˜ä½ï¼‰
  if (columnName.includes("æ™‚ä¾¡ç·é¡") || columnName.includes("å£²ä¸Šé«˜")) {
    return `${(value / 100000000).toFixed(2)}å„„å††`;
  }

  // å€ç‡
  if (columnName.includes("PBR") || columnName.includes("PER")) {
    return `${value.toFixed(2)}å€`;
  }

  return value.toString();
};
```

#### ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

```typescript
// useFilters.ts
import { useMemo, useState } from "react";
import { StockData } from "../types/stock";

export const useFilters = (data: StockData[]) => {
  const [searchTerm, setSearchTerm] = useState("");
  const [pbrMax, setPbrMax] = useState<number | null>(null);
  const [roeMin, setRoeMin] = useState<number | null>(null);

  const filteredData = useMemo(() => {
    return data.filter((stock) => {
      // æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿
      const matchesSearch =
        searchTerm === "" ||
        Object.values(stock).some((value) =>
          String(value).toLowerCase().includes(searchTerm.toLowerCase())
        );

      // PBRãƒ•ã‚£ãƒ«ã‚¿
      const matchesPBR =
        pbrMax === null || (stock.PBR && Number(stock.PBR) <= pbrMax);

      // ROEãƒ•ã‚£ãƒ«ã‚¿
      const matchesROE =
        roeMin === null || (stock.ROE && Number(stock.ROE) >= roeMin);

      return matchesSearch && matchesPBR && matchesROE;
    });
  }, [data, searchTerm, pbrMax, roeMin]);

  return {
    filteredData,
    searchTerm,
    setSearchTerm,
    pbrMax,
    setPbrMax,
    roeMin,
    setRoeMin,
  };
};
```

## 4. Docker ç’°å¢ƒã®æ§‹ç¯‰

### docker-compose.yml

```yaml
vservices:
  # Pythonãƒ‡ãƒ¼ã‚¿åé›†ã‚µãƒ¼ãƒ“ã‚¹
  python-service:
    build:
      context: .
      dockerfile: Dockerfile.fetch
    container_name: stock-data-collector
    env_file:
      - .env
    volumes:
      # æ ªå¼ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚¦ãƒ³ãƒˆ
      - ./stock_list:/app:rw
      # Exportãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å…±æœ‰
      - ./stock_list/Export:/app/Export:rw
    environment:
      - PYTHONUNBUFFERED=1
      - STOCK_FILE=${STOCK_FILE:-stocks_sample.json}
      - CHUNK_SIZE=${CHUNK_SIZE:-1000}
    restart: "no"

  # React ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹
  frontend-service:
    build:
      context: .
      dockerfile: Dockerfile.app
    container_name: stock-frontend
    env_file:
      - .env
    ports:
      - "${PORT}:80"
    environment:
      - NODE_ENV=${NODE_ENV:-production}
    command: >
      sh -c "echo 'Frontend running on http://localhost:${PORT}' && nginx -g 'daemon off;'"
```

# ä½¿ã„æ–¹

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆDockerï¼‰

### äº‹å‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ãªã‚‚ã®

- [Docker](https://docs.docker.com/get-started/get-docker/)

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/testkun08080/yfinance-jp-screener.git
cd yfinance-jp-screener

cp .env.example .env
# STOCK_FILEã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯"stocks_sample.json"ã«ãªã£ã¦ã„ã¾ã™ã€‚ å¿…ãšå…¨ã¦å–å¾—ã—ãŸã„å ´åˆã¯"stocks_all.json"ã¸å¤‰ãˆã¦ä¸‹ã•ã„

# Dockerèµ·å‹•ï¼ˆãƒ‡ãƒ¼ã‚¿åé›† â†’ ãƒ“ãƒ«ãƒ‰ â†’ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰

# ğŸ“¦ Python ãƒ‡ãƒ¼ã‚¿åé›†ãƒ“ãƒ«ãƒ‰ãƒ»å®Ÿè¡Œ
docker-compose build python-service
docker-compose run --rm python-service

# ğŸŒ ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒ“ãƒ«ãƒ‰ãƒ»èµ·å‹•
docker-compose build frontend-service
docker-compose up frontend-service

# ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¢ã‚¯ã‚»ã‚¹(ç’°å¢ƒå¤‰æ•°ã®PORTç•ªå·ã«ã‚ˆã‚Šã¾ã™)
open http://localhost:8080
```

**åˆå›èµ·å‹•æ™‚ã®æ³¨æ„:**
ãƒ‡ãƒ¼ã‚¿åé›†ã«ç´„ 4 æ™‚é–“ã‹ã‹ã‚Šã¾ã™ï¼ˆç´„ 3,795 ç¤¾ï¼‰ã€‚

## ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®å®Ÿè¡Œ

### äº‹å‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ãªã‚‚ã®

- [UV](https://docs.astral.sh/uv/getting-started/installation/)
- [nodejs](https://nodejs.org/en/download)

#### ãƒ‡ãƒ¼ã‚¿å–å¾—ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# 1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/yourusername/yfinance-jp-screener.git
cd yfinance-jp-screener/stock_list

# 2. Pythonç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆuvã‚’ä½¿ç”¨ï¼‰
uv sync

# 3. æ ªå¼ãƒªã‚¹ãƒˆå–å¾—ï¼ˆåˆå›ã®ã¿ï¼‰
uv run get_jp_stocklist.py

# 4. ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’å®Ÿè¡Œ
uv run sumalize.py stocks_sample.json   #ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆç”¨

#===ç´„1000ç¤¾ãšã¤ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰(æ¨å¥¨)===
# uv run sumalize.py stocks_1.json
# uv run sumalize.py stocks_2.json
# uv run sumalize.py stocks_3.json
# uv run sumalize.py stocks_4.json

#===ã™ã¹ã¦ã®éŠ˜æŸ„ã‚’å¯¾è±¡ã«ã—ãŸãƒ€ã‚¦ãƒ­ãƒ¼ãƒ‰===
# uv run sumalize.py stocks_all.json

# 6. CSVçµåˆ
uv run combine_latest_csv.py
```

#### ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# 1. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ç§»å‹•
cd ../stock_search

# 2. ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
npm install

# 3. é–‹ç™ºã‚µãƒ¼ãƒãƒ¼èµ·å‹•
npm run dev
# http://localhost:5173/ ã«ã‚¢ã‚¯ã‚»ã‚¹

# ã¾ãŸã¯ã€æœ¬ç•ªãƒ“ãƒ«ãƒ‰å¾Œã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
npm run build
npm run preview
# http://localhost:4173/ ã«ã‚¢ã‚¯ã‚»ã‚¹

```

---

### GitHub Actions ã§ã®è‡ªå‹•åé›†

:::message
**é‡è¦**: GitHub Actions ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ä»¥ä¸‹ã®è¨­å®šãŒå¿…è¦ã§ã™ï¼š

ãƒªãƒã‚¸ãƒˆãƒªã® **Settings** â†’ **Actions** â†’ **General** ã«ç§»å‹•

- âœ… **"Read and write permissions"** ã‚’é¸æŠ
- âœ… **"Allow GitHub Actions to create and approve pull requests"** ã«ãƒã‚§ãƒƒã‚¯

ã“ã®è¨­å®šã«ã‚ˆã‚Šã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒç”Ÿæˆã—ãŸ CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªãƒã‚¸ãƒˆãƒªã«ã‚³ãƒŸãƒƒãƒˆã§ãã¾ã™ã€‚
:::

**å®Ÿè¡Œæ‰‹é †:**

1. ãƒªãƒã‚¸ãƒˆãƒªã‚’**ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆ**ã§ãƒ•ã‚©ãƒ¼ã‚¯
2. Settings â†’ Actions ã§ä¸Šè¨˜æ¨©é™ã‚’è¨­å®š
3. Actions â†’ "Stock List Update" ã‚’å®Ÿè¡Œã—ã¦æ–°ã—ã„ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
4. Actions â†’ "ğŸ“Š Sequential Stock Fetch - Part 1" ã‚’å®Ÿè¡Œ
5. è‡ªå‹•é€£é–å®Ÿè¡Œï¼ˆPart 1 â†’ Part 2 â†’ Part 3 â†’ Part 4 â†’ CSV Combineï¼‰
6. ç´„ 3ã€œ4 æ™‚é–“å¾Œã«å…¨ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†
7. `stock_list/Export/YYYYMMDD_combined.csv` ã«çµåˆæ¸ˆã¿ CSV ãŒç”Ÿæˆã•ã‚Œã‚‹

**ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹æˆ:**

- `stock-list-update.yml` - JPX æ ªå¼ãƒªã‚¹ãƒˆæ›´æ–°ç”¨
- `stock-fetch-sequential-1~4.yml` - 4 æ®µéšãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆå„ 120 åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰
- `csv-combine-export.yml` - CSV çµåˆå‡¦ç†

# é‡è¦ãªæ³¨æ„äº‹é …

## âš ï¸ ãƒ‡ãƒ¼ã‚¿ã®å–ã‚Šæ‰±ã„ã«ã¤ã„ã¦

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯**å€‹äººåˆ©ç”¨ãƒ»ç ”ç©¶ãƒ»æ•™è‚²ç›®çš„**ã§ã®ã¿ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

:::message alert
**ãƒ‡ãƒ¼ã‚¿ã®å–ã‚Šæ‰±ã„ã¯ä»¥ä¸‹ã‚’å‚ç…§ãã ã•ã„**

1. Yahoo! Finance Terms of Service
   https://legal.yahoo.com/us/en/yahoo/terms/otos/index.html

2. Yahoo! Developer API Terms of Use
   https://policies.yahoo.com/us/en/yahoo/terms/product-atos/apiforydn/index.htm

3. Yahoo! æ¨©åˆ©é–¢ä¿‚ãƒšãƒ¼ã‚¸
   https://legal.yahoo.com/us/en/yahoo/permissions/requests/index.html

:::

- API ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å®ˆã‚Šã€éåº¦ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯é¿ã‘ã¦ãã ã•ã„
- å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºæ€§ã¯ä¿è¨¼ã•ã‚Œã¾ã›ã‚“

## å‚è€ƒãƒªãƒ³ã‚¯

- [yfinance GitHub Repository](https://github.com/ranaroussi/yfinance)
- [Yahoo Finance](https://finance.yahoo.com/)
- [æ—¥æœ¬å–å¼•æ‰€ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆJPXï¼‰](https://www.jpx.co.jp/)
- [ã‚ãŒæŠ•è³‡è¡“ï¼ˆAmazonï¼‰](https://amzn.to/3IEVRkq)

---

## ãã®ä»–

ã‚‚ã—ã“ã®è¨˜äº‹ãŒå½¹ç«‹ã£ãŸã‚‰ã€[ã‚³ãƒ¼ãƒ’ä¸€æ¯ã»ã©](https://buymeacoffee.com/testkun08080)ã‚‚ã‚‰ãˆã‚‹ã¨æœ€é«˜ã§ã™

æœ€å¾Œã¾ã§ãŠèª­ã¿ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚
ãã‚Œã§ã¯ ğŸ™

**å…è²¬äº‹é …:**
æŠ•è³‡åˆ¤æ–­ã¯è‡ªå·±è²¬ä»»ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚
